{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import getpass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import getpass\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "# loader = WebBaseLoader(\n",
    "#     web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "#     bs_kwargs=dict(\n",
    "#         parse_only=bs4.SoupStrainer(\n",
    "#             class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "#         )\n",
    "#     ),\n",
    "# )\n",
    "# docs = loader.load()\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# # Index chunks\n",
    "# _ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "\n",
    "def extract_data():\n",
    "    text_chunks = []\n",
    "    files = filter(lambda f: f.lower().endswith(\".pdf\"), os.listdir(\"./content_source/pdf\"))\n",
    "    file_list = list(files)\n",
    "    for file in file_list:\n",
    "        loader = PyPDFLoader(os.path.join('content_source', 'pdf', file))\n",
    "        text_chunks += loader.load_and_split(text_splitter=RecursiveCharacterTextSplitter(\n",
    "            chunk_size = 512,\n",
    "            chunk_overlap = 30,\n",
    "            length_function = len,\n",
    "            separators= [\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "        ))\n",
    "    vectorstore = FAISS.from_documents(documents=text_chunks, embedding=OpenAIEmbeddings(api_key=OPENAI_API_KEY))\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "vector_store = extract_data()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define prompt for question-answering\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content discusses advancements in machine learning and deep learning applications within the context of Directed Energy Deposition (DED) additive manufacturing. It focuses on using image analysis of melt pools to predict the quality of printed components, specifically by estimating deviations in printing focus. The work aims to enhance prediction accuracy by incorporating temporal information from preceding images.\n"
     ]
    }
   ],
   "source": [
    "# response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "response = graph.invoke({\"question\": \"What is this content all about?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is this content all about?',\n",
       " 'context': [Document(id='d92c7eed-4701-44f8-b069-301d436a9359', metadata={'producer': 'Adobe PSL 1.2e for Canon\\x00', 'creator': 'Canon iR-ADV C5250  PDF', 'creationdate': '2025-08-04T12:26:55+03:00', 'source': 'content_source/pdf/projeto_leonel-rev_kan1.pdf', 'total_pages': 39, 'page': 29, 'page_label': '30'}, page_content='58'),\n",
       "  Document(id='f17b0ab1-2940-4380-94b7-31273af7e9ba', metadata={'producer': 'Adobe PSL 1.2e for Canon\\x00', 'creator': 'Canon iR-ADV C5250  PDF', 'creationdate': '2025-08-04T12:26:55+03:00', 'source': 'content_source/pdf/projeto_leonel-rev_kan1.pdf', 'total_pages': 39, 'page': 38, 'page_label': '39'}, page_content='Elgendy, Mohamed. Deep learningfor vision systems. Simon and Schuster, 2020.\\nEra, Israt Zarin, et a1.,2023. \"Machine learning in Directed Energy Deposition (DED) additive manufacturing: A state-\\noÊthe-art review.\" Manufacturing Letters 35, pp. 689-700.\\nJogin, Manjunath,etal.,20l8. \"Feature extraction using convolution neural networks (Cl.lN) and deep learning.\" 2018\\n3rd IEEE international conference on recenl trends in electronics, information & communication technologt\\n(RTETCT). rEEE,2018.'),\n",
       "  Document(id='7d22fcda-8f73-4bcf-b715-e8b9a996de91', metadata={'producer': 'Adobe PSL 1.2e for Canon\\x00', 'creator': 'Canon iR-ADV C5250  PDF', 'creationdate': '2025-08-04T12:26:55+03:00', 'source': 'content_source/pdf/projeto_leonel-rev_kan1.pdf', 'total_pages': 39, 'page': 35, 'page_label': '36'}, page_content='monitoring. The review of the applications within each one of these 3 groups, leads to the conclusion that the melt pool\\nfeatures play a crucial role on the effort ofpredicting characteristics ofthe printed component.\\nThis work is intended to use images of melt pools during deposition to estimate deviations from printing focus, which\\nís recognized as a critical parameter to layer surface quality control Fig. I, from Ribeiro et al. (2023), was taken as main'),\n",
       "  Document(id='6b68de15-4916-4e43-8638-746a0a485e69', metadata={'producer': 'Adobe PSL 1.2e for Canon\\x00', 'creator': 'Canon iR-ADV C5250  PDF', 'creationdate': '2025-08-04T12:26:55+03:00', 'source': 'content_source/pdf/projeto_leonel-rev_kan1.pdf', 'total_pages': 39, 'page': 36, 'page_label': '37'}, page_content='3.3 Temporalinformation\\nThe base article performs regression on each image independently and achieves good results with a mean absolute\\nerror close to 0.25 mm. The current work aims to improve this metric by investigating the impact of adding information\\nabout the region right before the molten pool to be predicted. As the deposition is a continous process, the image n is\\ncloselly correlated with image n- I .')],\n",
       " 'answer': 'The content discusses advancements in machine learning and deep learning applications within the context of Directed Energy Deposition (DED) additive manufacturing. It focuses on using image analysis of melt pools to predict the quality of printed components, specifically by estimating deviations in printing focus. The work aims to enhance prediction accuracy by incorporating temporal information from preceding images.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
